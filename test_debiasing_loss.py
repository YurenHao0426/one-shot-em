#!/usr/bin/env python3
"""
æµ‹è¯•çº¯åè§å‡å°‘æŸå¤±å‡½æ•°
éªŒè¯ï¼šåªæœ€å°åŒ–ç”·å¥³ç†µå·®ï¼Œä¸åŒ…å«æ•´ä½“ç†µæœ€å°åŒ–
"""
import torch
import numpy as np
from losses.debiasing_loss import DebiasingLoss, gender_to_label

def test_debiasing_loss():
    """æµ‹è¯•çº¯åè§å‡å°‘æŸå¤±å‡½æ•°"""
    print("ğŸ§ª æµ‹è¯•çº¯åè§å‡å°‘æŸå¤±å‡½æ•°...")
    
    # åˆå§‹åŒ–æŸå¤±å‡½æ•°
    debiasing_l2 = DebiasingLoss(use_l1=False, scale_factor=1.0)
    debiasing_l1 = DebiasingLoss(use_l1=True, scale_factor=1.0)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 4
    vocab_size = 1000
    seq_len = 10
    
    # æ¨¡æ‹Ÿlogits
    torch.manual_seed(42)
    logits = torch.randn(batch_size, seq_len, vocab_size)
    attention_mask = torch.ones(batch_size, seq_len)
    prompt_lengths = torch.tensor([3, 4, 2, 5])  # ä¸åŒçš„prompté•¿åº¦
    
    # æ€§åˆ«æ ‡ç­¾: [ç”·, å¥³, ç”·, å¥³]
    gender_labels = torch.tensor([0, 1, 0, 1])
    
    print(f"ğŸ“Š æµ‹è¯•é…ç½®:")
    print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   åºåˆ—é•¿åº¦: {seq_len}")
    print(f"   è¯æ±‡é‡: {vocab_size}")
    print(f"   æ€§åˆ«åˆ†å¸ƒ: {gender_labels.tolist()}")
    
    # è®¡ç®—tokençº§ç†µ
    H_tok = debiasing_l2.compute_token_entropy(logits, attention_mask)
    print(f"   Tokenç†µå½¢çŠ¶: {H_tok.shape}")
    print(f"   Tokenç†µå‡å€¼: {H_tok.mean().item():.4f}")
    
    # è®¡ç®—æ ·æœ¬çº§ç†µ
    H_i = debiasing_l2.compute_sample_entropy(H_tok, prompt_lengths)
    print(f"   æ ·æœ¬ç†µ: {H_i.tolist()}")
    
    # è®¡ç®—ç»„ç†µ
    H_male, H_female = debiasing_l2.compute_group_entropy(H_i, gender_labels)
    print(f"   ç”·æ€§å¹³å‡ç†µ: {H_male.item():.4f}")
    print(f"   å¥³æ€§å¹³å‡ç†µ: {H_female.item():.4f}")
    print(f"   ç†µå·®è·: {abs(H_female - H_male).item():.4f}")
    
    # æµ‹è¯•L2æŸå¤±
    loss_l2, metrics_l2 = debiasing_l2.compute_debiasing_loss(H_i, gender_labels)
    print(f"\nğŸ“ˆ L2æŸå¤±ç»“æœ:")
    print(f"   æŸå¤±å€¼: {loss_l2.item():.6f}")
    print(f"   ç†µå·®è·: {metrics_l2['entropy_gap']:.6f}")
    print(f"   å¸¦ç¬¦å·å·®è·: {metrics_l2['entropy_gap_signed']:.6f}")
    print(f"   æ•´ä½“å¹³å‡ç†µ(ä»…ç›‘æ§): {metrics_l2['H_bar']:.6f}")
    
    # æµ‹è¯•L1æŸå¤±
    loss_l1, metrics_l1 = debiasing_l1.compute_debiasing_loss(H_i, gender_labels)
    print(f"\nğŸ“ˆ L1æŸå¤±ç»“æœ:")
    print(f"   æŸå¤±å€¼: {loss_l1.item():.6f}")
    print(f"   ç†µå·®è·: {metrics_l1['entropy_gap']:.6f}")
    
    # éªŒè¯æ•°å­¦å…³ç³»
    expected_l2 = (H_female - H_male) ** 2
    expected_l1 = torch.abs(H_female - H_male)
    
    print(f"\nğŸ” æ•°å­¦éªŒè¯:")
    print(f"   é¢„æœŸL2æŸå¤±: {expected_l2.item():.6f}")
    print(f"   å®é™…L2æŸå¤±: {loss_l2.item():.6f}")
    print(f"   L2è¯¯å·®: {abs(expected_l2.item() - loss_l2.item()):.8f}")
    
    print(f"   é¢„æœŸL1æŸå¤±: {expected_l1.item():.6f}")
    print(f"   å®é™…L1æŸå¤±: {loss_l1.item():.6f}")
    print(f"   L1è¯¯å·®: {abs(expected_l1.item() - loss_l1.item()):.8f}")
    
    # æµ‹è¯•ä¸å¹³è¡¡æ‰¹æ¬¡
    print(f"\nâš ï¸ æµ‹è¯•ä¸å¹³è¡¡æ‰¹æ¬¡:")
    unbalanced_labels = torch.tensor([0, 0, 0, 0])  # å…¨æ˜¯ç”·æ€§
    loss_unbalanced, metrics_unbalanced = debiasing_l2.compute_debiasing_loss(H_i, unbalanced_labels)
    print(f"   ä¸å¹³è¡¡æŸå¤±: {loss_unbalanced.item():.6f}")
    
    return True

def test_comparison_with_original():
    """å¯¹æ¯”åŸGEEæŸå¤±å’Œçº¯debiasingæŸå¤±çš„å·®å¼‚"""
    print(f"\nğŸ”„ å¯¹æ¯”æµ‹è¯•: åŸGEE vs çº¯Debiasing")
    
    # å¯¼å…¥åŸå§‹GEEæŸå¤±
    from losses.gee_loss import GEELoss
    
    # åˆå§‹åŒ–ä¸¤ç§æŸå¤±å‡½æ•°
    gee_loss = GEELoss(lambda_weight=3.0, use_l1=False)
    debiasing_loss = DebiasingLoss(use_l1=False, scale_factor=1.0)
    
    # åˆ›å»ºç›¸åŒçš„æµ‹è¯•æ•°æ®
    batch_size = 4
    H_i = torch.tensor([0.5, 0.8, 0.4, 0.9])  # æ ·æœ¬ç†µ
    gender_labels = torch.tensor([0, 1, 0, 1])  # [ç”·, å¥³, ç”·, å¥³]
    
    # è®¡ç®—åŸGEEæŸå¤±
    gee_total_loss, gee_metrics = gee_loss.compute_gee_loss(H_i, gender_labels)
    
    # è®¡ç®—çº¯debiasingæŸå¤±
    debiasing_total_loss, debiasing_metrics = debiasing_loss.compute_debiasing_loss(H_i, gender_labels)
    
    print(f"ğŸ“Š å¯¹æ¯”ç»“æœ:")
    print(f"   åŸGEEæ€»æŸå¤±: {gee_total_loss.item():.6f}")
    print(f"     - EMé¡¹: {gee_metrics['loss_em']:.6f}")
    print(f"     - Biasé¡¹: {gee_metrics['loss_bias']:.6f}")
    print(f"     - Î»æƒé‡: {gee_metrics['lambda_weight']}")
    
    print(f"   çº¯DebiasingæŸå¤±: {debiasing_total_loss.item():.6f}")
    print(f"     - åªæœ‰Biasé¡¹")
    
    print(f"   ğŸ“ å…³ç³»éªŒè¯:")
    print(f"     GEEçš„Biasé¡¹: {gee_metrics['loss_bias']:.6f}")
    print(f"     DebiasingæŸå¤±: {debiasing_total_loss.item():.6f}")
    print(f"     å·®å¼‚: {abs(gee_metrics['loss_bias'] - debiasing_total_loss.item()):.8f}")
    
    # éªŒè¯åªå…³æ³¨åè§å‡å°‘çš„æ•ˆæœ
    print(f"\nğŸ¯ æ•ˆæœåˆ†æ:")
    print(f"   åŸGEE: åŒæ—¶ä¼˜åŒ–ç†µæœ€å°åŒ– + åè§å‡å°‘")
    print(f"   çº¯Debiasing: åªä¼˜åŒ–åè§å‡å°‘")
    print(f"   é¢„æœŸ: Debiasingä¼šæ›´ä¸“æ³¨äºå¹³è¡¡ç”·å¥³ç†µå·®")

def simulate_training_progress():
    """æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ä¸­æŸå¤±çš„å˜åŒ–"""
    print(f"\nğŸ“ˆ æ¨¡æ‹Ÿè®­ç»ƒè¿›åº¦:")
    
    debiasing_loss = DebiasingLoss(use_l1=False, scale_factor=1.0)
    
    # æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤
    steps = [
        # [H_male, H_female] å¯¹
        ([0.8, 0.4], [0.6, 0.9]),  # åˆå§‹: å¾ˆå¤§å·®è·
        ([0.7, 0.5], [0.65, 0.75]), # æ­¥éª¤1: å·®è·ç¼©å°
        ([0.68, 0.62], [0.66, 0.68]), # æ­¥éª¤2: è¿›ä¸€æ­¥ç¼©å°
        ([0.67, 0.65], [0.66, 0.67]), # æ­¥éª¤3: æ¥è¿‘å¹³è¡¡
        ([0.66, 0.66], [0.665, 0.665]), # æ­¥éª¤4: å‡ ä¹ç›¸ç­‰
    ]
    
    print(f"ğŸ”„ æ¨¡æ‹Ÿç†æƒ³è®­ç»ƒè½¨è¿¹:")
    for i, (male_entropies, female_entropies) in enumerate(steps):
        # æ„é€ æ ·æœ¬ç†µ
        H_i = torch.tensor(male_entropies + female_entropies)
        gender_labels = torch.tensor([0, 0, 1, 1])  # 2ç”·2å¥³
        
        loss, metrics = debiasing_loss.compute_debiasing_loss(H_i, gender_labels)
        
        gap_direction = "ğŸ“‰" if i == 0 else ("ğŸ“‰" if metrics['entropy_gap'] < prev_gap else "ğŸ“ˆ")
        
        print(f"   {gap_direction} Step {i}: loss={loss.item():.6f} | "
              f"gap={metrics['entropy_gap']:.6f} | "
              f"H_male={metrics['H_male']:.4f} | "
              f"H_female={metrics['H_female']:.4f}")
        
        prev_gap = metrics['entropy_gap']
    
    print(f"âœ… é¢„æœŸç»“æœ: æŸå¤±å’Œç†µå·®è·éƒ½åº”è¯¥æŒç»­ä¸‹é™")

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æµ‹è¯•çº¯åè§å‡å°‘æŸå¤±å‡½æ•°")
    
    # åŸºç¡€åŠŸèƒ½æµ‹è¯•
    success = test_debiasing_loss()
    
    if success:
        print("\nâœ… åŸºç¡€æµ‹è¯•é€šè¿‡ï¼")
        
        # å¯¹æ¯”æµ‹è¯•
        test_comparison_with_original()
        
        # è®­ç»ƒæ¨¡æ‹Ÿ
        simulate_training_progress()
        
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print(f"ğŸ“‹ æ€»ç»“:")
        print(f"   âœ… çº¯åè§å‡å°‘æŸå¤±å‡½æ•°å·¥ä½œæ­£å¸¸")
        print(f"   âœ… åªå…³æ³¨ç”·å¥³ç†µå·®ï¼Œä¸åŒ…å«EMé¡¹")
        print(f"   âœ… æ”¯æŒL1å’ŒL2ä¸¤ç§æŸå¤±å½¢å¼")
        print(f"   âœ… æ•°å­¦è®¡ç®—æ­£ç¡®")
        print(f"   ğŸ¯ å¯ä»¥å¼€å§‹çº¯debiasingè®­ç»ƒäº†ï¼")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼") 