# ğŸ¯ ä¸‹ä¸€é˜¶æ®µå®æ–½æŒ‡å—ï¼šBenchmarkæµ‹è¯•ä¸æ•°æ®å®Œå–„

## ğŸ‰ å½“å‰æˆæœå›é¡¾

âœ… **çº¯åè§å‡å°‘è®­ç»ƒæˆåŠŸ**
- ç†µå·®è·ä» 33.2% â†’ 1.6% (æ”¹å–„95.3%)
- è®­ç»ƒæ­¥æ•°ï¼šä»…12æ­¥è¾¾åˆ°ç›®æ ‡
- æ‰¹æ¬¡å¹³è¡¡ï¼šå®Œç¾çš„1ç”·1å¥³åˆ†å¸ƒ
- æ–¹æ³•éªŒè¯ï¼šè¯æ˜äº†å»é™¤EMé¡¹çš„æœ‰æ•ˆæ€§

## ğŸš€ ä¸‹ä¸€é˜¶æ®µç›®æ ‡

### 1. **éªŒè¯çœŸå®åœºæ™¯æ•ˆæœ**
- åœ¨çœŸå®benchmarkä¸Šæµ‹è¯•åè§å‡å°‘æ•ˆæœ
- éªŒè¯ä»£ç /æ•°å­¦èƒ½åŠ›æ˜¯å¦ä¿æŒ
- å»ºç«‹æ ‡å‡†åŒ–è¯„ä¼°æµç¨‹

### 2. **æ‰©å±•åˆ°çœŸå®æ•°æ®**
- ä½¿ç”¨Numinaæ•°å­¦æ•°æ®é›†(460MB+)
- å¢å¼ºæ•°æ®å¤„ç†èƒ½åŠ›
- å»ºç«‹å·¥ä¸šçº§è®­ç»ƒpipeline

### 3. **å»ºç«‹è¯„ä¼°æ ‡å‡†**
- å¤šç»´benchmarkè¯„ä¼°
- æ€§èƒ½ä¿æŒåº¦åˆ†æ
- å¯å¤ç°çš„è¯„ä¼°æµç¨‹

## ğŸ› ï¸ æ–°å¢å·¥å…·ä¸è„šæœ¬

### æ ¸å¿ƒè¯„ä¼°å·¥å…·
```bash
ğŸ“ æ–°å¢æ–‡ä»¶ç»“æ„ï¼š
â”œâ”€â”€ create_bias_benchmark.py     # åˆ›å»ºåè§è¯„ä¼°benchmark
â”œâ”€â”€ run_bias_evaluation.py       # è¿è¡Œæ¨¡å‹å¯¹æ¯”è¯„ä¼°  
â”œâ”€â”€ enhance_gee_processor.py     # å¢å¼ºæ•°æ®å¤„ç†å™¨
â”œâ”€â”€ start_next_phase.sh          # ä¸€é”®å¯åŠ¨ä¸‹ä¸€é˜¶æ®µ
â””â”€â”€ comprehensive_evaluation_plan.md  # è¯¦ç»†å®æ–½è®¡åˆ’
```

### 1. åè§è¯„ä¼°Benchmark
```bash
python create_bias_benchmark.py
# åŠŸèƒ½ï¼š
# - åˆ›å»ºæ•°å­¦ã€ä»£ç ã€èŒä¸šåœºæ™¯çš„æ€§åˆ«å¹³è¡¡æµ‹è¯•é›†
# - ç”ŸæˆCSVå’ŒJSONæ ¼å¼æ•°æ®
# - ç»Ÿè®¡æ ·æœ¬åˆ†å¸ƒå’Œç±»åˆ«
```

### 2. æ¨¡å‹å¯¹æ¯”è¯„ä¼°
```bash
python run_bias_evaluation.py \
    --original_model "Qwen/Qwen2.5-Math-1.5B-Instruct" \
    --debiased_model "checkpoints/.../final" \
    --output_dir "results/bias_comparison"
    
# åŠŸèƒ½ï¼š
# - å¯¹æ¯”åŸå§‹æ¨¡å‹ vs å»åè§æ¨¡å‹
# - ç”Ÿæˆè¯¦ç»†è¯„ä¼°æŠ¥å‘Šå’Œå¯è§†åŒ–
# - è®¡ç®—æ”¹è¿›ç¨‹åº¦å’Œæ€§èƒ½ä¿æŒåº¦
```

### 3. å¢å¼ºæ•°æ®å¤„ç†å™¨
```bash
python enhance_gee_processor.py
# åŠŸèƒ½ï¼š
# - å¤„ç†Numinaæ•°å­¦æ¨ç†æ•°æ®
# - æ™ºèƒ½æ€§åˆ«åŒ–æ–‡æœ¬è½¬æ¢
# - åˆ›å»ºå¹³è¡¡æ•°æ®é›†
```

### 4. ä¸€é”®å¯åŠ¨è„šæœ¬
```bash
./start_next_phase.sh
# åŠŸèƒ½ï¼š
# - è‡ªåŠ¨åŒ–æ•´ä¸ªè¯„ä¼°æµç¨‹
# - äº¤äº’å¼é€‰æ‹©è¯„ä¼°é¡¹ç›®
# - ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
```

## ğŸ“Š å¯ç”¨Benchmarkåˆ—è¡¨

### ä»£ç èƒ½åŠ›è¯„ä¼°
- âœ… **HumanEval**: ä»£ç ç”ŸæˆåŸºå‡†
- âœ… **MBPP**: Pythonä»£ç ç†è§£  
- âœ… **BigCodeBench**: ç»¼åˆä»£ç èƒ½åŠ›
- âœ… **LiveCodeBench**: æœ€æ–°ä»£ç æŒ‘æˆ˜

### æ•°å­¦æ¨ç†è¯„ä¼°  
- âœ… **GSM8K**: å°å­¦æ•°å­¦åº”ç”¨é¢˜
- âœ… **MATH**: ç«èµ›æ•°å­¦é—®é¢˜
- âœ… **AIME**: æ•°å­¦ç«èµ›
- âœ… **College Math**: å¤§å­¦æ•°å­¦

### åè§è¯„ä¼°
- âœ… **WinoGenderé£æ ¼**: èŒä¸šåˆ»æ¿å°è±¡
- âœ… **æ•°å­¦é—®é¢˜æ€§åˆ«åŒ–**: åº”ç”¨é¢˜ä¸­çš„æ€§åˆ«è§’è‰²
- âœ… **ä»£ç åœºæ™¯**: ç¼–ç¨‹ä»»åŠ¡ä¸­çš„æ€§åˆ«å¼•ç”¨

## ğŸ“‚ å¯ç”¨æ•°æ®èµ„æº

### çœŸå®è®­ç»ƒæ•°æ®
```bash
dataset/
â”œâ”€â”€ numina/          # 460MB+ æ•°å­¦æ¨ç†æ•°æ®
â”‚   â”œâ”€â”€ numina_00.parquet (48MB)
â”‚   â”œâ”€â”€ numina_01.parquet (48MB)
â”‚   â””â”€â”€ ... (10ä¸ªæ–‡ä»¶)
â””â”€â”€ 1shot_rlvr/      # å¼ºåŒ–å­¦ä¹ æ•°æ®
    â”œâ”€â”€ pi1_r128.parquet
    â””â”€â”€ pi1_r1280.parquet
```

### è¯„ä¼°æ•°æ®
```bash
Qwen2.5-Eval/evaluation/data/
â”œâ”€â”€ gsm8k/test.jsonl      # æ•°å­¦åº”ç”¨é¢˜
â”œâ”€â”€ math/test.jsonl       # ç«èµ›æ•°å­¦  
â”œâ”€â”€ aime24/test.jsonl     # æ•°å­¦ç«èµ›
â””â”€â”€ ... (æ›´å¤šbenchmark)
```

## ğŸ¯ ç«‹å³å¼€å§‹

### å¿«é€Ÿå¯åŠ¨ (æ¨è)
```bash
# ä¸€é”®è¿è¡Œæ‰€æœ‰è¯„ä¼°
./start_next_phase.sh
```

### åˆ†æ­¥æ‰§è¡Œ
```bash
# 1. åˆ›å»ºbenchmark
python create_bias_benchmark.py

# 2. è¿è¡Œåè§è¯„ä¼°
python run_bias_evaluation.py \
    --debiased_model checkpoints/Qwen2.5-Math-1.5B-Instruct/colab_pure_debiasing/final

# 3. ä»£ç èƒ½åŠ›æµ‹è¯•
python code_eval/OpenCodeEval/main.py \
    --model_path checkpoints/.../final \
    --benchmark HumanEval

# 4. æ•°å­¦èƒ½åŠ›æµ‹è¯•  
python Qwen2.5-Eval/evaluation/math_eval.py \
    --model_path checkpoints/.../final \
    --data_path Qwen2.5-Eval/evaluation/data/gsm8k/test.jsonl
```

## ğŸ“ˆ é¢„æœŸç»“æœ

### æˆåŠŸæ ‡å‡†
- ğŸ¯ **åè§å‡å°‘**: ç†µå·®è· < 2% (å·²è¾¾æˆ1.6%)
- ğŸ¯ **æ€§èƒ½ä¿æŒ**: ä¸»è¦benchmarkä¸‹é™ < 5%
- ğŸ¯ **è®­ç»ƒæ•ˆç‡**: æ¯”åŸGEEæ–¹æ³•å¿«50%+

### è¯„ä¼°æŠ¥å‘Š
è¿è¡Œåä¼šç”Ÿæˆï¼š
```bash
results/
â”œâ”€â”€ bias_comparison/
â”‚   â”œâ”€â”€ detailed_results.json       # è¯¦ç»†è¯„ä¼°æ•°æ®
â”‚   â”œâ”€â”€ bias_comparison_plot.png    # å¯è§†åŒ–å›¾è¡¨
â”‚   â””â”€â”€ evaluation_summary.json     # è¯„ä¼°æ‘˜è¦
â”œâ”€â”€ humaneval/                      # ä»£ç è¯„ä¼°ç»“æœ
â””â”€â”€ gsm8k/                         # æ•°å­¦è¯„ä¼°ç»“æœ
```

## ğŸ”® åç»­è·¯çº¿å›¾

### Week 1: åŸºç¡€éªŒè¯
- [ ] å®Œæˆåè§benchmarkè¯„ä¼°
- [ ] éªŒè¯ä»£ç /æ•°å­¦èƒ½åŠ›ä¿æŒ
- [ ] å»ºç«‹è¯„ä¼°åŸºçº¿

### Week 2: çœŸå®æ•°æ®è®­ç»ƒ
- [ ] ä½¿ç”¨Numinaæ•°æ®é‡æ–°è®­ç»ƒ
- [ ] å¯¹æ¯”åˆæˆæ•°æ® vs çœŸå®æ•°æ®æ•ˆæœ
- [ ] ä¼˜åŒ–æ•°æ®å¤„ç†pipeline

### Week 3: å¤§è§„æ¨¡è¯„ä¼°
- [ ] å…¨é¢benchmarkæµ‹è¯•
- [ ] æ€§èƒ½æƒè¡¡åˆ†æ
- [ ] æ’°å†™æŠ€æœ¯æŠ¥å‘Š

### Week 4: æ–¹æ³•æ¨å¹¿
- [ ] æ‰©å±•åˆ°æ›´å¤§æ¨¡å‹(7B/72B)
- [ ] å»ºç«‹æ ‡å‡†åŒ–debiasingæµç¨‹
- [ ] å‡†å¤‡è®ºæ–‡/å¼€æºå‘å¸ƒ

## ğŸ’¡ å…³é”®æ´å¯Ÿ

1. **çº¯åè§å‡å°‘çš„ä¼˜åŠ¿å·²éªŒè¯**
   - æ”¶æ•›é€Ÿåº¦å¿«(12æ­¥ vs 50+æ­¥)
   - æ•ˆæœæ˜¾è‘—(95%+åè§å‡å°‘)
   - å®ç°ç®€å•(æ— éœ€Î»æƒé‡è°ƒèŠ‚)

2. **ä¸‹ä¸€æ­¥é‡ç‚¹**
   - éªŒè¯çœŸå®åœºæ™¯æ³›åŒ–èƒ½åŠ›
   - ç¡®ä¿æ€§èƒ½ä¸ä¸‹é™
   - å»ºç«‹å¯å¤ç°pipeline

3. **å•†ä¸šåŒ–æ½œåŠ›**
   - é€‚åˆèµ„æºå—é™ç¯å¢ƒ
   - å¿«é€Ÿåè§ä¿®æ­£
   - å¯é›†æˆåˆ°ç°æœ‰è®­ç»ƒæµç¨‹

## ğŸ‰ å¼€å§‹è¡ŒåŠ¨

```bash
# ç«‹å³å¼€å§‹ä¸‹ä¸€é˜¶æ®µï¼
./start_next_phase.sh
```

ä½ çš„çº¯åè§å‡å°‘æ–¹æ³•å·²ç»å–å¾—çªç ´æ€§è¿›å±•ï¼Œç°åœ¨æ˜¯éªŒè¯å’Œæ¨å¹¿çš„æ—¶å€™äº†ï¼ğŸš€
