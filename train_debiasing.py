#!/usr/bin/env python3
"""
çº¯åè§å‡å°‘è®­ç»ƒè„šæœ¬
ç›®æ ‡ï¼šåªæœ€å°åŒ–ç”·å¥³é—´ç†µå·®ï¼Œä¸è¿›è¡Œæ•´ä½“ç†µæœ€å°åŒ–
"""
import argparse
import os
import torch
import torch.nn.functional as F
from torch.optim import AdamW
import pandas as pd
import numpy as np
from pathlib import Path

import wandb
from accelerate import Accelerator, DeepSpeedPlugin
from accelerate.utils import set_seed
from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
import sys
sys.path.append('.')
from dataset.gee_processor import GEEProcessor
from losses.debiasing_loss import DebiasingLoss, gender_to_label
from smart_balanced_dataloader import create_smart_balanced_dataloader

os.environ.setdefault("NCCL_TIMEOUT", "2700")
os.environ.setdefault("TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC", "2700")

def parse_args():
    parser = argparse.ArgumentParser()
    # Debiasingç›¸å…³å‚æ•°
    parser.add_argument('--scale_factor', type=float, default=1.0, help='Debiasing loss scale factor')
    parser.add_argument('--use_l1', action='store_true', help='Use L1 loss instead of L2')
    parser.add_argument('--target_gap', type=float, default=0.01, help='Target entropy gap for early stopping')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--model_name', type=str, default='Qwen2.5-Math-1.5B-Instruct', help='Model name')
    parser.add_argument('--model_path', type=str, required=True, help='Model path')
    parser.add_argument('--effective_batch', type=int, default=4, help='Global batch size')
    parser.add_argument('--micro_batch_size', type=int, default=2, help='Micro batch size (must >=2 for balance)')
    parser.add_argument('--learning_rate', type=float, default=1e-5, help='Learning rate (lower for debiasing)')
    parser.add_argument('--max_steps', type=int, default=20, help='Maximum training steps')
    parser.add_argument('--sample_temp', type=float, default=0.7, help='Generation temperature')
    
    # è¿è¡Œå‚æ•°
    parser.add_argument('--run_name', type=str, default='pure_debiasing', help='Run name')
    parser.add_argument('--wandb_project', type=str, default='debiasing-only', help='W&B project name')
    parser.add_argument('--use_test_data', action='store_true', help='Use synthetic test data')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    parser.add_argument('--log_steps', type=int, default=1, help='Logging frequency')
    parser.add_argument('--save_steps', type=int, default=10, help='Save frequency')
    
    return parser.parse_args()

def main():
    args = parse_args()
    set_seed(args.seed)
    
    # å¼ºåˆ¶æ£€æŸ¥batch_size
    if args.micro_batch_size < 2:
        print("âŒ é”™è¯¯: micro_batch_sizeå¿…é¡»>=2æ‰èƒ½ä¿è¯æ€§åˆ«å¹³è¡¡ï¼")
        print("è¯·ä½¿ç”¨: --micro_batch_size 2 æˆ–æ›´å¤§")
        return
    
    # DeepSpeedé…ç½®
    ds_config = {
        "train_micro_batch_size_per_gpu": args.micro_batch_size,
        "train_batch_size": args.effective_batch,
        "gradient_accumulation_steps": max(1, args.effective_batch // args.micro_batch_size),
        "bf16": {"enabled": True},
        "zero_optimization": {
            "stage": 2, 
            "offload_optimizer": {"device": "cpu"}, 
            "offload_param": {"device": "none"}
        },
        "gradient_clipping": 1.0,
    }
    
    accelerator = Accelerator(
        mixed_precision="bf16",
        gradient_accumulation_steps=max(1, args.effective_batch // args.micro_batch_size),
        deepspeed_plugin=DeepSpeedPlugin(hf_ds_config=ds_config)
    )
    
    print = accelerator.print
    print(f"ğŸ¯ å¼€å§‹çº¯åè§å‡å°‘è®­ç»ƒ - {args.run_name}")
    print(f"ğŸ“Š é…ç½®ä¿¡æ¯:")
    print(f"   æ‰¹æ¬¡å¤§å°: micro={args.micro_batch_size}, effective={args.effective_batch}")
    print(f"   ç¼©æ”¾å› å­: {args.scale_factor}")
    print(f"   ç›®æ ‡ç†µå·®: {args.target_gap}")
    print(f"   å­¦ä¹ ç‡: {args.learning_rate}")
    print(f"   âš ï¸  æ³¨æ„: åªè¿›è¡Œåè§å‡å°‘ï¼Œä¸è¿›è¡Œç†µæœ€å°åŒ–")

    # åŠ è½½æ¨¡å‹
    model_path = args.model_path
    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
    config.use_cache = False
    model = AutoModelForCausalLM.from_pretrained(model_path, config=config, trust_remote_code=True)
    model.gradient_checkpointing_enable()
    
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # åˆå§‹åŒ–çº¯åè§å‡å°‘æŸå¤±å‡½æ•°
    gee_processor = GEEProcessor(tokenizer)
    debiasing_loss_fn = DebiasingLoss(use_l1=args.use_l1, scale_factor=args.scale_factor)

    if accelerator.is_main_process:
        wandb.init(project=args.wandb_project, name=args.run_name, config=vars(args))

    # å‡†å¤‡æ•°æ®
    if args.use_test_data:
        print("ğŸ“Š ä½¿ç”¨åˆæˆæµ‹è¯•æ•°æ®...")
        train_data = gee_processor.create_test_data(num_samples=100)
        
        # æ£€æŸ¥æ•°æ®å¹³è¡¡æ€§
        male_count = sum(1 for item in train_data if item['gender'] == 'male')
        female_count = sum(1 for item in train_data if item['gender'] == 'female')
        print(f"åŸå§‹æ•°æ®: male={male_count}, female={female_count}")
        
        # åˆ›å»ºæ™ºèƒ½å¹³è¡¡çš„æ•°æ®åŠ è½½å™¨
        train_loader = create_smart_balanced_dataloader(
            train_data, 
            batch_size=args.micro_batch_size, 
            num_batches=args.max_steps + 5
        )
    else:
        print("âŒ è¯·ä½¿ç”¨ --use_test_data è¿›è¡Œæµ‹è¯•")
        return

    optimizer = AdamW(model.parameters(), lr=args.learning_rate)
    model, optimizer = accelerator.prepare(model, optimizer)
    
    print(f"ğŸ¯ å¼€å§‹è®­ç»ƒ...")
    print(f"   ç›®æ ‡: æœ€å°åŒ– |H_female - H_male|")
    print(f"   ç†æƒ³ç»“æœ: entropy_gap â†’ {args.target_gap}")
    
    # å¼€å§‹è®­ç»ƒ
    model.train()
    best_gap = float('inf')
    converged_steps = 0
    
    for step, batch in enumerate(train_loader, start=1):
        if step > args.max_steps:
            print(f"ğŸ›‘ è¾¾åˆ°æœ€å¤§æ­¥æ•° {args.max_steps}ï¼Œè®­ç»ƒç»“æŸ")
            break
        
        with accelerator.accumulate(model):
            try:
                # éªŒè¯æ‰¹æ¬¡å¹³è¡¡æ€§
                male_count = sum(1 for g in batch["gender"] if g == 'male')
                female_count = sum(1 for g in batch["gender"] if g == 'female')
                
                if male_count == 0 or female_count == 0:
                    print(f"ğŸ’¥ Step {step}: æ‰¹æ¬¡ä¸å¹³è¡¡ï¼male={male_count}, female={female_count}")
                    continue
                
                # å‡†å¤‡è¾“å…¥
                inputs = tokenizer(
                    batch["input"], 
                    return_tensors="pt", 
                    padding="longest", 
                    truncation=True, 
                    max_length=1024
                ).to(accelerator.device)
                
                # ç”Ÿæˆå›ç­”
                with torch.no_grad():
                    gen_ids = accelerator.unwrap_model(model).generate(
                        **inputs,
                        max_new_tokens=128,
                        do_sample=True,
                        top_p=0.95,
                        temperature=args.sample_temp,
                        synced_gpus=True,
                        repetition_penalty=1.15,
                        pad_token_id=tokenizer.pad_token_id,
                        use_cache=False
                    )
                
                # å‡†å¤‡å®Œæ•´åºåˆ—
                seq = torch.cat([inputs.input_ids, gen_ids[:, inputs.input_ids.shape[1]:]], dim=1)
                pad_mask = seq.ne(tokenizer.pad_token_id)
                prompt_lengths = pad_mask[:, :inputs.input_ids.shape[1]].sum(-1)
                
                # è®¡ç®—logitså’Œç†µ
                logits = model(seq, attention_mask=pad_mask).logits
                H_tok = debiasing_loss_fn.compute_token_entropy(logits, pad_mask)
                H_i = debiasing_loss_fn.compute_sample_entropy(H_tok, prompt_lengths)
                
                # å‡†å¤‡æ€§åˆ«æ ‡ç­¾
                gender_labels = torch.tensor([
                    gender_to_label(g) for g in batch["gender"]
                ], device=accelerator.device)
                
                # è®¡ç®—çº¯åè§å‡å°‘æŸå¤±
                loss, metrics = debiasing_loss_fn.compute_debiasing_loss(H_i, gender_labels)
                
                # åå‘ä¼ æ’­
                accelerator.backward(loss)
                accelerator.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                optimizer.zero_grad()
                
                # æ£€æŸ¥æ”¶æ•›
                current_gap = metrics['entropy_gap']
                if current_gap < best_gap:
                    best_gap = current_gap
                    converged_steps = 0
                else:
                    converged_steps += 1
                
                # æ—©åœæ£€æŸ¥
                if current_gap <= args.target_gap:
                    print(f"ğŸ‰ è¾¾åˆ°ç›®æ ‡ç†µå·® {args.target_gap}ï¼å½“å‰: {current_gap:.6f}")
                    print(f"åœ¨ç¬¬ {step} æ­¥å®ç°ç›®æ ‡")
                    break
                
                # æ—¥å¿—è®°å½•
                if accelerator.is_main_process:
                    if step % args.log_steps == 0:
                        gap_direction = "ğŸ“‰" if current_gap <= best_gap else "ğŸ“ˆ"
                        print(f"{gap_direction} Step {step} | loss={loss.item():.6f} | "
                              f"gap={current_gap:.6f} | "
                              f"H_male={metrics['H_male']:.6f} | "
                              f"H_female={metrics['H_female']:.6f} | "
                              f"æ‰¹æ¬¡[{male_count}M,{female_count}F]")
                        
                        # æ·»åŠ è¿›åº¦æŒ‡æ ‡
                        progress = max(0, min(1, (0.1 - current_gap) / 0.1)) * 100  # å‡è®¾ä»0.1å¼€å§‹
                        metrics['progress_percent'] = progress
                        metrics['best_gap'] = best_gap
                        
                        wandb.log({"step": step, **metrics})
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if accelerator.is_main_process and step % args.save_steps == 0:
                    ckpt = Path(f"checkpoints/{args.model_name}/{args.run_name}") / f"step_{step}"
                    ckpt.mkdir(parents=True, exist_ok=True)
                    accelerator.unwrap_model(model).save_pretrained(ckpt, safe_serialization=True)
                    tokenizer.save_pretrained(ckpt)
                    print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {ckpt}")
                
            except Exception as e:
                print(f"âŒ è®­ç»ƒæ­¥éª¤ {step} å‡ºé”™: {e}")
                continue

    if accelerator.is_main_process:
        print(f"\nğŸ‰ çº¯åè§å‡å°‘è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“Š æœ€ç»ˆç»“æœ:")
        print(f"   æœ€ä½³ç†µå·®: {best_gap:.6f}")
        print(f"   ç›®æ ‡ç†µå·®: {args.target_gap}")
        
        if best_gap <= args.target_gap:
            print("âœ… æˆåŠŸè¾¾åˆ°åè§å‡å°‘ç›®æ ‡ï¼")
        elif best_gap <= 0.05:
            print("âš ï¸ åè§æ˜¾è‘—å‡å°‘ï¼Œæ¥è¿‘ç›®æ ‡")
        else:
            print("âŒ åè§å‡å°‘æ•ˆæœæœ‰é™ï¼Œå¯èƒ½éœ€è¦æ›´å¤šè®­ç»ƒæ­¥æ•°æˆ–è°ƒæ•´å‚æ•°")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final = Path(f"checkpoints/{args.model_name}/{args.run_name}") / "final"
        final.mkdir(parents=True, exist_ok=True)
        accelerator.unwrap_model(model).save_pretrained(final, safe_serialization=True)
        tokenizer.save_pretrained(final)
        print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final}")
        
        wandb.finish()

if __name__ == "__main__":
    main() 