#!/usr/bin/env python3
"""
GEEè®­ç»ƒé€»è¾‘æµ‹è¯•è„šæœ¬
æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹è€Œä¸éœ€è¦çœŸå®æ¨¡å‹
"""

import sys
import os
import torch
import numpy as np
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('.')

from dataset.gee_processor import GEEProcessor
from losses.gee_loss import GEELoss, gender_to_label

class MockTokenizer:
    def __init__(self):
        self.pad_token_id = 0
        self.eos_token = '<|endoftext|>'
        self.pad_token = self.eos_token
        
    def apply_chat_template(self, messages, tokenize=False, add_generation_prompt=True):
        return messages[0]["content"]
    
    def __call__(self, texts, return_tensors=None, padding=None, truncation=None, max_length=None):
        # æ¨¡æ‹Ÿtokenization
        batch_size = len(texts)
        seq_len = 50  # å›ºå®šåºåˆ—é•¿åº¦ç”¨äºæµ‹è¯•
        
        return {
            'input_ids': torch.randint(1, 1000, (batch_size, seq_len)),
            'attention_mask': torch.ones(batch_size, seq_len)
        }

class MockModel:
    def __init__(self):
        self.device = 'cpu'
    
    def __call__(self, input_ids, attention_mask=None):
        batch_size, seq_len = input_ids.shape
        vocab_size = 1000
        
        # æ¨¡æ‹Ÿlogitsè¾“å‡º
        logits = torch.randn(batch_size, seq_len, vocab_size)
        
        class MockOutput:
            def __init__(self, logits):
                self.logits = logits
        
        return MockOutput(logits)
    
    def generate(self, input_ids, attention_mask=None, max_new_tokens=50, **kwargs):
        batch_size, prompt_len = input_ids.shape
        # æ¨¡æ‹Ÿç”Ÿæˆæ–°çš„token
        new_tokens = torch.randint(1, 1000, (batch_size, max_new_tokens))
        return torch.cat([input_ids, new_tokens], dim=1)

def test_gee_training_logic():
    """æµ‹è¯•GEEè®­ç»ƒé€»è¾‘"""
    print("="*60)
    print("æµ‹è¯•GEEè®­ç»ƒé€»è¾‘")
    print("="*60)
    
    # åˆå§‹åŒ–ç»„ä»¶
    tokenizer = MockTokenizer()
    model = MockModel()
    gee_processor = GEEProcessor(tokenizer)
    gee_loss_fn = GEELoss(lambda_weight=3.0, use_l1=False)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    train_data = gee_processor.create_test_data(num_samples=20)
    print(f"ç”Ÿæˆè®­ç»ƒæ•°æ®: {len(train_data)} æ¡")
    
    # æ¨¡æ‹Ÿè®­ç»ƒå¾ªç¯
    batch_size = 4
    num_steps = 5
    
    print(f"\nå¼€å§‹æ¨¡æ‹Ÿè®­ç»ƒ ({num_steps} æ­¥)...")
    
    for step in range(1, num_steps + 1):
        # åˆ›å»ºbatch
        batch_data = train_data[(step-1)*batch_size:step*batch_size]
        if len(batch_data) < batch_size:
            # å¾ªç¯ä½¿ç”¨æ•°æ®
            batch_data = train_data[:batch_size]
        
        batch = {
            "input": [item["input"] for item in batch_data],
            "gender": [item["gender"] for item in batch_data]
        }
        
        # æ¨¡æ‹Ÿtokenization
        inputs = tokenizer(batch["input"])
        
        # æ¨¡æ‹Ÿç”Ÿæˆ
        gen_ids = model.generate(**inputs, max_new_tokens=20)
        
        # å‡†å¤‡å®Œæ•´åºåˆ—
        seq = gen_ids[:, :100]  # é™åˆ¶é•¿åº¦ç”¨äºæµ‹è¯•
        prompt_lengths = torch.tensor([inputs['input_ids'].shape[1]] * batch_size)
        
        # è®¡ç®—logitså’Œç†µ
        mock_output = model(seq)
        logits = mock_output.logits
        
        # è®¡ç®—GEEæŸå¤±
        H_tok = gee_loss_fn.compute_token_entropy(logits)
        H_i = gee_loss_fn.compute_sample_entropy(H_tok, prompt_lengths)
        
        # å‡†å¤‡æ€§åˆ«æ ‡ç­¾
        gender_labels = torch.tensor([gender_to_label(g) for g in batch["gender"]])
        
        # è®¡ç®—æŸå¤±
        loss, metrics = gee_loss_fn.compute_gee_loss(H_i, gender_labels)
        
        # æ‰“å°è®­ç»ƒæ—¥å¿—
        print(f"Step {step} | loss={loss.item():.6f} | "
              f"entropy_gap={metrics['entropy_gap']:.6f} | "
              f"H_male={metrics['H_male']:.6f} | "
              f"H_female={metrics['H_female']:.6f}")
        
        # éªŒè¯æŸå¤±è®¡ç®—
        assert not torch.isnan(loss), "æŸå¤±ä¸ºNaN"
        assert loss.item() > 0, "æŸå¤±åº”è¯¥ä¸ºæ­£å€¼"
        assert 'entropy_gap' in metrics, "ç¼ºå°‘entropy_gapæŒ‡æ ‡"
    
    print("âœ“ GEEè®­ç»ƒé€»è¾‘æµ‹è¯•é€šè¿‡")

def test_different_lambdas():
    """æµ‹è¯•ä¸åŒlambdaå€¼çš„å½±å“"""
    print("\n" + "="*60)
    print("æµ‹è¯•ä¸åŒlambdaå€¼çš„å½±å“")
    print("="*60)
    
    tokenizer = MockTokenizer()
    model = MockModel()
    gee_processor = GEEProcessor(tokenizer)
    
    # æµ‹è¯•ä¸åŒçš„lambdaå€¼
    lambda_values = [0.0, 1.0, 3.0, 5.0]
    
    # åˆ›å»ºå›ºå®šçš„æµ‹è¯•æ•°æ®
    batch_size = 4
    seq_len = 50
    vocab_size = 1000
    
    logits = torch.randn(batch_size, seq_len, vocab_size)
    prompt_lengths = torch.tensor([20, 20, 20, 20])
    gender_labels = torch.tensor([0, 1, 0, 1])  # male, female, male, female
    
    print("Lambdaå€¼å¯¹æŸå¤±çš„å½±å“:")
    print("Lambda\tEM Loss\tBias Loss\tTotal Loss\tEntropy Gap")
    print("-" * 60)
    
    for lambda_val in lambda_values:
        gee_loss_fn = GEELoss(lambda_weight=lambda_val, use_l1=False)
        
        H_tok = gee_loss_fn.compute_token_entropy(logits)
        H_i = gee_loss_fn.compute_sample_entropy(H_tok, prompt_lengths)
        loss, metrics = gee_loss_fn.compute_gee_loss(H_i, gender_labels)
        
        print(f"{lambda_val:.1f}\t{metrics['loss_em']:.4f}\t"
              f"{metrics['loss_bias']:.4f}\t{metrics['loss_total']:.4f}\t"
              f"{metrics['entropy_gap']:.4f}")
    
    print("âœ“ Lambdaå€¼æµ‹è¯•é€šè¿‡")

def test_l1_vs_l2():
    """æµ‹è¯•L1å’ŒL2æŸå¤±çš„å·®å¼‚"""
    print("\n" + "="*60)
    print("æµ‹è¯•L1å’ŒL2æŸå¤±çš„å·®å¼‚")
    print("="*60)
    
    # åˆ›å»ºå›ºå®šçš„æµ‹è¯•æ•°æ®
    batch_size = 4
    seq_len = 50
    vocab_size = 1000
    
    logits = torch.randn(batch_size, seq_len, vocab_size)
    prompt_lengths = torch.tensor([20, 20, 20, 20])
    gender_labels = torch.tensor([0, 1, 0, 1])
    
    # æµ‹è¯•L2ç‰ˆæœ¬
    gee_loss_l2 = GEELoss(lambda_weight=3.0, use_l1=False)
    H_tok = gee_loss_l2.compute_token_entropy(logits)
    H_i = gee_loss_l2.compute_sample_entropy(H_tok, prompt_lengths)
    loss_l2, metrics_l2 = gee_loss_l2.compute_gee_loss(H_i, gender_labels)
    
    # æµ‹è¯•L1ç‰ˆæœ¬
    gee_loss_l1 = GEELoss(lambda_weight=3.0, use_l1=True)
    loss_l1, metrics_l1 = gee_loss_l1.compute_gee_loss(H_i, gender_labels)
    
    print(f"L2æŸå¤±: {metrics_l2['loss_total']:.6f} (bias: {metrics_l2['loss_bias']:.6f})")
    print(f"L1æŸå¤±: {metrics_l1['loss_total']:.6f} (bias: {metrics_l1['loss_bias']:.6f})")
    print(f"ç†µå·®è·: {metrics_l2['entropy_gap']:.6f}")
    
    print("âœ“ L1 vs L2æµ‹è¯•é€šè¿‡")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹GEEè®­ç»ƒé€»è¾‘æµ‹è¯•...")
    
    try:
        test_gee_training_logic()
        test_different_lambdas()
        test_l1_vs_l2()
        
        print("\n" + "="*60)
        print("æ‰€æœ‰è®­ç»ƒé€»è¾‘æµ‹è¯•é€šè¿‡ï¼âœ“")
        print("="*60)
        print("\næ ¸å¿ƒåŠŸèƒ½éªŒè¯:")
        print("âœ… æ•°æ®å¤„ç†æµç¨‹æ­£å¸¸")
        print("âœ… æŸå¤±å‡½æ•°è®¡ç®—æ­£ç¡®")
        print("âœ… è®­ç»ƒå¾ªç¯é€»è¾‘æ­£ç¡®")
        print("âœ… ä¸åŒå‚æ•°é…ç½®æœ‰æ•ˆ")
        print("\nğŸ¯ å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥è¿›è¡ŒçœŸå®æ¨¡å‹è®­ç»ƒï¼")
        
    except Exception as e:
        print(f"\næµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 