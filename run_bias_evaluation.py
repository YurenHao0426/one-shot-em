#!/usr/bin/env python3
"""
è¿è¡Œåè§è¯„ä¼°å¯¹æ¯”
æ¯”è¾ƒåŸå§‹æ¨¡å‹ vs çº¯debiasingæ¨¡å‹çš„åè§å‡å°‘æ•ˆæœ
"""
import argparse
import json
import pandas as pd
from pathlib import Path
import sys
sys.path.append('.')

from evaluation.gee_evaluator import GEEEvaluator

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--original_model', type=str, default="Qwen/Qwen2.5-Math-1.5B-Instruct", help='Original model path')
    parser.add_argument('--debiased_model', type=str, required=True, help='Debiased model path')
    parser.add_argument('--test_data', type=str, default="bias_evaluation_benchmark.json", help='Test data file')
    parser.add_argument('--output_dir', type=str, default="results/bias_comparison", help='Output directory')
    parser.add_argument('--max_new_tokens', type=int, default=128, help='Max tokens for generation')
    return parser.parse_args()

def main():
    args = parse_args()
    
    print(f"ğŸ¯ å¼€å§‹åè§è¯„ä¼°å¯¹æ¯”...")
    print(f"   åŸå§‹æ¨¡å‹: {args.original_model}")
    print(f"   å»åè§æ¨¡å‹: {args.debiased_model}")
    print(f"   æµ‹è¯•æ•°æ®: {args.test_data}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    with open(args.test_data, 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    print(f"ğŸ“Š åŠ è½½äº† {len(test_data)} ä¸ªæµ‹è¯•æ ·æœ¬")
    
    # å‡†å¤‡è¯„ä¼°
    models_to_compare = {
        'Original': args.original_model,
        'Pure_Debiasing': args.debiased_model
    }
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨ï¼ˆä½¿ç”¨åŸå§‹æ¨¡å‹ï¼‰
    print(f"\nğŸ”§ åˆå§‹åŒ–è¯„ä¼°å™¨...")
    evaluator = GEEEvaluator(args.original_model)
    
    # è¿è¡Œå¯¹æ¯”è¯„ä¼°
    print(f"\nğŸ“ˆ å¼€å§‹æ¨¡å‹å¯¹æ¯”è¯„ä¼°...")
    results = evaluator.compare_models(models_to_compare, test_data)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_file = output_dir / 'detailed_results.json'
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜: {results_file}")
    
    # ç”Ÿæˆå¯è§†åŒ–
    print(f"\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    plot_file = output_dir / 'bias_comparison_plot.png'
    evaluator.plot_results(results, save_path=str(plot_file))
    
    # æ‰“å°æ‘˜è¦
    evaluator.print_summary(results)
    
    # è®¡ç®—æ”¹è¿›ç¨‹åº¦
    original_gap = results['Original']['entropy_gap']
    debiased_gap = results['Pure_Debiasing']['entropy_gap']
    improvement = ((original_gap - debiased_gap) / original_gap) * 100
    
    print(f"\nï¿½ï¿½ åè§å‡å°‘æ•ˆæœ:")
    print(f"   åŸå§‹æ¨¡å‹ç†µå·®è·: {original_gap:.6f}")
    print(f"   å»åè§æ¨¡å‹ç†µå·®è·: {debiased_gap:.6f}")
    print(f"   æ”¹è¿›ç¨‹åº¦: {improvement:.1f}%")
    
    # ç”ŸæˆæŠ¥å‘Šæ‘˜è¦
    summary = {
        'evaluation_summary': {
            'original_entropy_gap': original_gap,
            'debiased_entropy_gap': debiased_gap, 
            'improvement_percentage': improvement,
            'test_samples': len(test_data),
            'models_compared': list(models_to_compare.keys())
        },
        'recommendation': 'Excellent' if improvement > 90 else ('Good' if improvement > 70 else ('Moderate' if improvement > 50 else 'Needs Improvement'))
    }
    
    summary_file = output_dir / 'evaluation_summary.json'
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“‹ è¯„ä¼°æ‘˜è¦å·²ä¿å­˜: {summary_file}")
    print(f"ğŸ¯ è¯„ä¼°å®Œæˆï¼æŸ¥çœ‹ {output_dir} ç›®å½•è·å–å®Œæ•´ç»“æœ")

if __name__ == "__main__":
    main()
